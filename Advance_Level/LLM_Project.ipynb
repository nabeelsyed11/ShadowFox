{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecbaa6a",
   "metadata": {},
   "source": [
    "# ðŸ§  LLM Project â€” Language Model Exploration\n",
    "\n",
    "This notebook implements and analyzes language models (LMs). It supports DistilBERT / BERT (masked LM) and GPT-2 (causal LM). The notebook contains:\n",
    "- Installation & setup\n",
    "- Loading one of (DistilBERT, BERT, GPT-2)\n",
    "- Text classification (sentiment) and Cloze tasks (masked LM)\n",
    "- Text generation (GPT-2)\n",
    "- Pseudo-perplexity evaluation (approximate for causal LM)\n",
    "- Visualizations: bar charts, attention heatmap / token importance\n",
    "- Research questions and conclusion template\n",
    "\n",
    "> Make sure you have internet on first run so Hugging Face can download models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14728f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q transformers torch matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9cbbcf",
   "metadata": {},
   "source": [
    "## Imports & utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    pipeline,\n",
    ")\n",
    "# Device config\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd07a6",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "Change `MODEL_NAME` to one of: `distilbert-base-uncased`, `bert-base-uncased`, `gpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "MODEL_NAME = 'distilbert-base-uncased'  # or 'bert-base-uncased' or 'gpt2'\n",
    "\n",
    "is_masked_lm = 'bert' in MODEL_NAME or 'distilbert' in MODEL_NAME\n",
    "is_causal_lm = MODEL_NAME.startswith('gpt')\n",
    "print('Model:', MODEL_NAME, 'MaskedLM:', is_masked_lm, 'CausalLM:', is_causal_lm)\n",
    "\n",
    "# Load tokenizer and model/backbone (backbone for attention visualization)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if is_masked_lm:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME).to(device)\n",
    "    # backbone to get attentions (some models share weights)\n",
    "    backbone = AutoModel.from_pretrained(MODEL_NAME, output_attentions=True).to(device)\n",
    "elif is_causal_lm:\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "    backbone = None\n",
    "else:\n",
    "    raise ValueError('Unsupported model choice')\n",
    "\n",
    "model.eval()\n",
    "print('Loaded model and tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41832575",
   "metadata": {},
   "source": [
    "## Text classification (Sentiment) â€” quick pipeline\n",
    "- We'll use Hugging Face `pipeline('sentiment-analysis')` for a quick classification baseline.\n",
    "- This pipeline will load its own lightweight model; it's independent of the LM you chose above but useful for the assignment section on text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sentiment = pipeline('sentiment-analysis')\n",
    "    examples = [\n",
    "        'I love this product! It works great and exceeded my expectations.',\n",
    "        'This was a waste of money. Very disappointed.'\n",
    "    ]\n",
    "    for ex in examples:\n",
    "        print(ex)\n",
    "        print(sentiment(ex))\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print('Sentiment pipeline failed:', e)\n",
    "    print('You can still run model-specific tasks below.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe20eaf",
   "metadata": {},
   "source": [
    "## Cloze task (Masked LM) â€” example probes\n",
    "If you selected a masked LM (BERT/DistilBERT), we'll run a few cloze-style sentences with `[MASK]` and show top-k predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_mask_predictions(text, k=5):\n",
    "    \"\"\"Return top-k token predictions for the first mask in `text` (uses tokenizer's mask token).\"\"\"\n",
    "    if tokenizer.mask_token is None:\n",
    "        raise ValueError('Selected tokenizer has no mask token')\n",
    "    t = text.replace('[MASK]', tokenizer.mask_token)\n",
    "    inputs = tokenizer(t, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    mask_indices = (inputs['input_ids'][0] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "    if len(mask_indices) == 0:\n",
    "        raise ValueError('No mask in input')\n",
    "    idx = mask_indices[0].item()\n",
    "    probs = torch.softmax(logits[0, idx], dim=-1)\n",
    "    topk = torch.topk(probs, k)\n",
    "    tokens = [tokenizer.decode([i]) for i in topk.indices]\n",
    "    scores = topk.values.tolist()\n",
    "    return list(zip(tokens, scores))\n",
    "\n",
    "if is_masked_lm:\n",
    "    probes = [\n",
    "        ('The capital of France is [MASK].', 'paris'),\n",
    "        ('An apple a day keeps the [MASK] away.', 'doctor'),\n",
    "        ('The quick brown fox jumps over the lazy [MASK].', 'dog'),\n",
    "    ]\n",
    "    for text, answer in probes:\n",
    "        try:\n",
    "            preds = top_mask_predictions(text, k=5)\n",
    "            print('\\nPrompt:', text)\n",
    "            print('Top-5:', preds)\n",
    "            print('Expected answer:', answer)\n",
    "        except Exception as e:\n",
    "            print('Error on', text, e)\n",
    "else:\n",
    "    print('Cloze task is for masked LMs (BERT/DistilBERT). Change MODEL_NAME to a masked model to run this.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b36ef",
   "metadata": {},
   "source": [
    "## Text generation (GPT-2)\n",
    "If you selected GPT-2, we'll generate continuations for a few prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_gpt2(prompt, max_new_tokens=50, temperature=0.8, top_p=0.9, num_return_sequences=1):\n",
    "    gen = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if device=='cuda' else -1)\n",
    "    out = gen(prompt, max_length=len(tokenizer(prompt)['input_ids']) + max_new_tokens,\n",
    "              do_sample=True, temperature=temperature, top_p=top_p,\n",
    "              num_return_sequences=num_return_sequences)\n",
    "    return [o['generated_text'] for o in out]\n",
    "\n",
    "if is_causal_lm:\n",
    "    prompts = [\n",
    "        'Once upon a time in a small village,',\n",
    "        'Advances in machine learning have shown that',\n",
    "        'The future of education will likely be',\n",
    "    ]\n",
    "    for p in prompts:\n",
    "        print('\\nPrompt:', p)\n",
    "        try:\n",
    "            gens = generate_with_gpt2(p, max_new_tokens=40, num_return_sequences=2)\n",
    "            for g in gens:\n",
    "                print('-', g)\n",
    "        except Exception as e:\n",
    "            print('Generation failed:', e)\n",
    "else:\n",
    "    print('Text generation section: change MODEL_NAME to a GPT-2 model to run generation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f74aa",
   "metadata": {},
   "source": [
    "## Pseudo-perplexity evaluation (approximate)\n",
    "For causal LMs we compute a rough token-level pseudo-perplexity by scoring each token conditioned on the prefix (this is not the true corpus PPL but useful for relative comparison on short sentences). For masked LMs we compute a simple cloze top-1 accuracy on our small probe set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_perplexity_causal(text, model, tokenizer):\n",
    "    \"\"\"Approximate perplexity for causal models by scoring each token with the prefix.\"\"\"\n",
    "    enc = tokenizer(text, return_tensors='pt').to(device)\n",
    "    input_ids = enc['input_ids'][0]\n",
    "    nlls = []\n",
    "    # we iterate from 1..N-1 (score each token given previous tokens)\n",
    "    for i in range(1, input_ids.size(0)):\n",
    "        prefix = input_ids[:i].unsqueeze(0)\n",
    "        target_id = input_ids[i].unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(prefix)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            nll = -log_probs[0, target_id]\n",
    "            nlls.append(nll.item())\n",
    "    if len(nlls) == 0:\n",
    "        return float('nan')\n",
    "    return float(np.exp(np.mean(nlls)))\n",
    "\n",
    "def masked_lm_cloze_accuracy(probes):\n",
    "    correct = 0\n",
    "    details = []\n",
    "    for text, answer in probes:\n",
    "        try:\n",
    "            preds = top_mask_predictions(text, k=5)\n",
    "            top1 = preds[0][0].strip().lower()\n",
    "            ok = (answer.strip().lower() in top1) or (answer.strip().lower() == top1)\n",
    "            correct += int(ok)\n",
    "            details.append((text, answer, preds, ok))\n",
    "        except Exception as e:\n",
    "            details.append((text, answer, str(e), False))\n",
    "    acc = correct / len(probes)\n",
    "    return acc, details\n",
    "\n",
    "if is_causal_lm:\n",
    "    texts = [\n",
    "        'Natural language processing enables machines to understand human language.',\n",
    "        'Transformers have changed the field of deep learning for sequence modeling.'\n",
    "    ]\n",
    "    for t in texts:\n",
    "        try:\n",
    "            ppl = pseudo_perplexity_causal(t, model, tokenizer)\n",
    "            print(f'Text: \"{t}\" -> Pseudo-PPL: {ppl:.2f}')\n",
    "        except Exception as e:\n",
    "            print('Pseudo-PPL failed on', t, e)\n",
    "elif is_masked_lm:\n",
    "    probes = [\n",
    "        ('The capital of France is [MASK].', 'paris'),\n",
    "        ('An apple a day keeps the [MASK] away.', 'doctor'),\n",
    "        ('He drank a cup of [MASK] in the morning.', 'coffee')\n",
    "    ]\n",
    "    acc, details = masked_lm_cloze_accuracy(probes)\n",
    "    print(f'Masked LM Top-1 Cloze Accuracy on {len(probes)} probes: {acc:.2%}')\n",
    "    for d in details:\n",
    "        print(d)\n",
    "else:\n",
    "    print('No evaluation run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6524d1",
   "metadata": {},
   "source": [
    "## Visualization: Performance bar chart\n",
    "Example showing how you might compare models. Replace `metrics` with real computed values from experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict, title='Model performance'):\n",
    "    labels = list(metrics_dict.keys())\n",
    "    values = [metrics_dict[k] for k in labels]\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.bar(labels, values)\n",
    "    plt.ylim(0, max(1.0, max(values)*1.1))\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example (replace with real values)\n",
    "example_metrics = {\n",
    "    'DistilBERT_cloze_acc': 0.67,\n",
    "    'BERT_cloze_acc': 0.72,\n",
    "    'GPT2_pseudo_ppl (lower better)': 45.3\n",
    "}\n",
    "plot_metrics({'DistilBERT': 0.67, 'BERT': 0.72, 'GPT-2 (pseudo-PPL scaled)': 0.55},\n",
    "             title='Example comparison (illustrative)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca3ab4",
   "metadata": {},
   "source": [
    "## Attention / Token importance visualization (for masked LMs)\n",
    "We visualize the attention matrix of the last layer's first head as a heatmap. This gives an interpretability snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_masked_lm and backbone is not None:\n",
    "    sentence = 'Transformers are powerful models for natural language processing.'\n",
    "    inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        out = backbone(**inputs)\n",
    "    # out.attentions: tuple (num_layers, batch, num_heads, seq, seq)\n",
    "    if hasattr(out, 'attentions') and out.attentions is not None:\n",
    "        atts = out.attentions  # tuple\n",
    "        last = atts[-1][0, 0].cpu().numpy()  # head 0\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        plt.figure(figsize=(6,5))\n",
    "        plt.imshow(last, aspect='auto')\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
    "        plt.yticks(range(len(tokens)), tokens)\n",
    "        plt.title('Attention (last layer, head 0)')\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No attention available from backbone for this model.')\n",
    "else:\n",
    "    print('Attention visualization is available only for masked LMs (BERT/DistilBERT).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eaef14",
   "metadata": {},
   "source": [
    "## Research questions & experimental ideas\n",
    "- **Contextual understanding**: How well does the LM fill cloze tasks that require world knowledge vs. local syntax?\n",
    "- **Robustness**: How sensitive is cloze accuracy / generation to small paraphrases, typos, or domain shift?\n",
    "- **Creativity & coherence**: For GPT-2, evaluate generated text quality for different temperatures and nucleus sampling (top-p).\n",
    "- **Interpretability**: Do attention heads focus on syntax (function words) or key content words? Visualize per-head attention.\n",
    "- **Bias and safety**: Probe models for stereotypical outputs and add filters / post-processing to mitigate.\n",
    "\n",
    "Suggested experiments:\n",
    "1. Create a larger curated probe set (100+ cloze sentences) across several domains (general, medical, legal, conversational) and compute cloze accuracy.\n",
    "2. Compare pseudo-perplexity of GPT-2 across domains.\n",
    "3. Try few-shot prompting (for GPT-2) and measure improvements in desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cd96d",
   "metadata": {},
   "source": [
    "## Conclusion & next steps\n",
    "- Summarize your findings here (e.g., DistilBERT achieved X% top-1 cloze accuracy on small probe; GPT-2 produced fluent continuations but had a pseudo-PPL of Y).\n",
    "- Limitations: small probe set, approximate metrics, no fine-tuning.\n",
    "- Next steps: expand probes, fine-tune on domain data (if compute permits), compare more models, add human evaluation for generation quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
